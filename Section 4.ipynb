{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T18:17:40.247828Z",
     "start_time": "2025-01-09T18:12:04.421168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('./https_training.csv')\n",
    "test_df = pd.read_csv('./https_test.csv')\n",
    "\n",
    "# Define target variables and columns to exclude\n",
    "exclude_columns = ['_s_bytes_uniq']\n",
    "targets = ['_s_bytes_all']\n",
    "drop_columns = ['c_ip', 'time']\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Preprocess for each regression target\n",
    "for target_column in targets:\n",
    "    print(f\"\\n--- Regression for {target_column} ---\\n\")\n",
    "\n",
    "    # Prepare training and test datasets\n",
    "    X_train = train_df.drop(columns=[target_column] + exclude_columns + drop_columns, errors='ignore')\n",
    "    y_train = train_df[target_column]\n",
    "    X_test = test_df.drop(columns=[target_column] + exclude_columns + drop_columns, errors='ignore')\n",
    "    y_test = test_df[target_column]\n",
    "\n",
    "    # Handle categorical data and one-hot encode\n",
    "    X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "    X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "    # Align training and test features\n",
    "    X_train, X_test = X_train.align(X_test, join='inner', axis=1)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=20)  # Adjust based on dataset size and explained variance\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    explained_variance = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"Total Explained Variance by PCA: {explained_variance:.2%}\")\n",
    "\n",
    "    # Define models\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Random Forest\": RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    }\n",
    "\n",
    "    # Train and evaluate models\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate performance\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "        results.append({\n",
    "            \"Target\": target_column,\n",
    "            \"Model\": model_name,\n",
    "            \"Train MAE\": train_mae,\n",
    "            \"Test MAE\": test_mae,\n",
    "            \"Train MSE\": train_mse,\n",
    "            \"Test MSE\": test_mse,\n",
    "            \"Train R²\": train_r2,\n",
    "            \"Test R²\": test_r2\n",
    "        })\n",
    "\n",
    "# Output results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n--- Final Regression Results ---\")\n",
    "print(results_df)\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Regression for _s_bytes_all ---\n",
      "\n",
      "Total Explained Variance by PCA: 59.08%\n",
      "Training Linear Regression...\n",
      "Training Random Forest...\n",
      "Training Gradient Boosting...\n",
      "\n",
      "--- Final Regression Results ---\n",
      "         Target              Model      Train MAE       Test MAE  \\\n",
      "0  _s_bytes_all  Linear Regression  214666.368645  221423.380653   \n",
      "1  _s_bytes_all      Random Forest   56432.280016  118437.419141   \n",
      "2  _s_bytes_all  Gradient Boosting  131820.078071  173382.084477   \n",
      "\n",
      "      Train MSE      Test MSE  Train R²   Test R²  \n",
      "0  7.432915e+11  2.663221e+12  0.953974  0.969203  \n",
      "1  1.899029e+11  5.170408e+13  0.988241  0.402098  \n",
      "2  3.291760e+11  4.715343e+13  0.979617  0.454721  \n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T18:33:50.186926Z",
     "start_time": "2025-01-09T18:28:08.639166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define parameter grids for hyperparameter tuning\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [5, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tuning and evaluating models\n",
    "tuned_results = []\n",
    "\n",
    "for target_column in targets:\n",
    "    print(f\"\\n--- Hyperparameter Tuning for {target_column} ---\\n\")\n",
    "\n",
    "    # Prepare training and test datasets\n",
    "    X_train = train_df.drop(columns=[target_column] + exclude_columns + drop_columns, errors='ignore')\n",
    "    y_train = train_df[target_column]\n",
    "    X_test = test_df.drop(columns=[target_column] + exclude_columns + drop_columns, errors='ignore')\n",
    "    y_test = test_df[target_column]\n",
    "\n",
    "    X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "    X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "    X_train, X_test = X_train.align(X_test, join='inner', axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    pca = PCA(n_components=20, svd_solver='randomized')\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    # Sample dataset for faster tuning\n",
    "    X_train_sample = X_train[:10000]\n",
    "    y_train_sample = y_train[:10000]\n",
    "\n",
    "    for model_name, param_grid in param_grids.items():\n",
    "        if param_grid:\n",
    "            print(f\"Tuning {model_name}...\")\n",
    "            model = {\n",
    "                \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "                \"Gradient Boosting\": GradientBoostingRegressor(random_state=42)\n",
    "            }.get(model_name)\n",
    "\n",
    "            # Perform random search\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=model,\n",
    "                param_distributions=param_grid,\n",
    "                n_iter=20,  # Number of parameter combinations to try\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                cv=3,\n",
    "                verbose=1,\n",
    "                n_jobs=-1,\n",
    "                random_state=42\n",
    "            )\n",
    "            random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "            # Evaluate the best model\n",
    "            best_model = random_search.best_estimator_\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "            train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "            test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "            test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "            tuned_results.append({\n",
    "                \"Target\": target_column,\n",
    "                \"Model\": model_name,\n",
    "                \"Best Params\": random_search.best_params_,\n",
    "                \"Train MAE\": train_mae,\n",
    "                \"Test MAE\": test_mae,\n",
    "                \"Train MSE\": train_mse,\n",
    "                \"Test MSE\": test_mse,\n",
    "                \"Train R²\": train_r2,\n",
    "                \"Test R²\": test_r2\n",
    "            })\n",
    "\n",
    "# Output tuned results\n",
    "tuned_results_df = pd.DataFrame(tuned_results)\n",
    "print(\"\\n--- Tuned Model Results ---\")\n",
    "print(tuned_results_df)\n"
   ],
   "id": "b44795714ce3ac76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameter Tuning for _s_bytes_all ---\n",
      "\n",
      "Tuning Random Forest...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Tuning Gradient Boosting...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "--- Tuned Model Results ---\n",
      "         Target              Model  \\\n",
      "0  _s_bytes_all      Random Forest   \n",
      "1  _s_bytes_all  Gradient Boosting   \n",
      "\n",
      "                                         Best Params      Train MAE  \\\n",
      "0  {'n_estimators': 50, 'min_samples_split': 2, '...  133148.250421   \n",
      "1  {'n_estimators': 200, 'max_depth': 7, 'learnin...  133165.945705   \n",
      "\n",
      "        Test MAE     Train MSE      Test MSE  Train R²   Test R²  \n",
      "0  165689.155137  3.578929e+12  6.203320e+13  0.778387  0.282653  \n",
      "1  166609.672766  3.875635e+12  6.338125e+13  0.760014  0.267064  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T18:45:34.364988Z",
     "start_time": "2025-01-09T18:40:55.352469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the datasets\n",
    "train_df_rtt = pd.read_csv('./https_training.csv')  # Changed DataFrame name\n",
    "test_df_rtt = pd.read_csv('./https_test.csv')  # Changed DataFrame name\n",
    "\n",
    "# Define target variable and columns to exclude\n",
    "rtt_target = '_s_rtt_avg'  # Target variable\n",
    "exclude_rtt_columns = ['_s_rtt_min', '_s_rtt_max', '_s_rtt_stddev']  # Exclude RTT-related columns\n",
    "drop_rtt_columns = ['c_ip', 'time']  # Drop non-numeric or irrelevant columns\n",
    "\n",
    "# Remove rows where target is 0\n",
    "train_df_rtt = train_df_rtt[train_df_rtt[rtt_target] > 0]\n",
    "test_df_rtt = test_df_rtt[test_df_rtt[rtt_target] > 0]\n",
    "\n",
    "# Prepare training and test datasets\n",
    "X_train_rtt = train_df_rtt.drop(columns=[rtt_target] + exclude_rtt_columns + drop_rtt_columns, errors='ignore')\n",
    "y_train_rtt = train_df_rtt[rtt_target]\n",
    "X_test_rtt = test_df_rtt.drop(columns=[rtt_target] + exclude_rtt_columns + drop_rtt_columns, errors='ignore')\n",
    "y_test_rtt = test_df_rtt[rtt_target]\n",
    "\n",
    "# Handle categorical data and one-hot encode\n",
    "X_train_rtt = pd.get_dummies(X_train_rtt, drop_first=True)\n",
    "X_test_rtt = pd.get_dummies(X_test_rtt, drop_first=True)\n",
    "\n",
    "# Align training and test features\n",
    "X_train_rtt, X_test_rtt = X_train_rtt.align(X_test_rtt, join='inner', axis=1)\n",
    "\n",
    "# Scale the features\n",
    "rtt_scaler = StandardScaler()  # Changed variable name\n",
    "X_train_rtt = rtt_scaler.fit_transform(X_train_rtt)\n",
    "X_test_rtt = rtt_scaler.transform(X_test_rtt)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "rtt_pca = PCA(n_components=15)  # Changed PCA variable name\n",
    "X_train_rtt = rtt_pca.fit_transform(X_train_rtt)\n",
    "X_test_rtt = rtt_pca.transform(X_test_rtt)\n",
    "rtt_explained_variance = rtt_pca.explained_variance_ratio_.sum()\n",
    "print(f\"Total Explained Variance by PCA for _s_rtt_avg: {rtt_explained_variance:.2%}\")\n",
    "\n",
    "# Define models\n",
    "rtt_models = {  # Changed dictionary name\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "}\n",
    "\n",
    "# Initialize results storage\n",
    "rtt_results = []  # Changed results variable name\n",
    "\n",
    "# Train and evaluate models\n",
    "for model_name, model in rtt_models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(X_train_rtt, y_train_rtt)\n",
    "    y_train_pred_rtt = model.predict(X_train_rtt)\n",
    "    y_test_pred_rtt = model.predict(X_test_rtt)\n",
    "\n",
    "    # Evaluate performance\n",
    "    train_mae_rtt = mean_absolute_error(y_train_rtt, y_train_pred_rtt)\n",
    "    test_mae_rtt = mean_absolute_error(y_test_rtt, y_test_pred_rtt)\n",
    "    train_mse_rtt = mean_squared_error(y_train_rtt, y_train_pred_rtt)\n",
    "    test_mse_rtt = mean_squared_error(y_test_rtt, y_test_pred_rtt)\n",
    "    train_r2_rtt = r2_score(y_train_rtt, y_train_pred_rtt)\n",
    "    test_r2_rtt = r2_score(y_test_rtt, y_test_pred_rtt)\n",
    "\n",
    "    rtt_results.append({\n",
    "        \"Target\": rtt_target,\n",
    "        \"Model\": model_name,\n",
    "        \"Train MAE\": train_mae_rtt,\n",
    "        \"Test MAE\": test_mae_rtt,\n",
    "        \"Train MSE\": train_mse_rtt,\n",
    "        \"Test MSE\": test_mse_rtt,\n",
    "        \"Train R²\": train_r2_rtt,\n",
    "        \"Test R²\": test_r2_rtt\n",
    "    })\n",
    "\n",
    "# Output results\n",
    "rtt_results_df = pd.DataFrame(rtt_results)  # Changed DataFrame name\n",
    "print(\"\\n--- Regression Results for _s_rtt_avg ---\")\n",
    "print(rtt_results_df)\n"
   ],
   "id": "28bc78176b25ad26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Explained Variance by PCA for _s_rtt_avg: 53.88%\n",
      "Training Linear Regression...\n",
      "Training Random Forest...\n",
      "Training Gradient Boosting...\n",
      "\n",
      "--- Regression Results for _s_rtt_avg ---\n",
      "       Target              Model   Train MAE     Test MAE     Train MSE  \\\n",
      "0  _s_rtt_avg  Linear Regression  839.796127  1085.696927  8.793663e+06   \n",
      "1  _s_rtt_avg      Random Forest  405.511200   667.611432  2.791106e+06   \n",
      "2  _s_rtt_avg  Gradient Boosting  618.625463   803.020388  6.493763e+06   \n",
      "\n",
      "       Test MSE  Train R²   Test R²  \n",
      "0  9.897405e+06  0.056837  0.017351  \n",
      "1  6.577647e+06  0.700640  0.346948  \n",
      "2  8.296926e+06  0.303513  0.176252  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T19:03:02.638161Z",
     "start_time": "2025-01-09T18:58:39.764370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define parameter grids for hyperparameter tuning\n",
    "rtt_param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [5, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tuning and evaluating models for _s_rtt_avg\n",
    "rtt_tuned_results = []\n",
    "\n",
    "# Prepare training and test datasets for _s_rtt_avg\n",
    "rtt_target_column = '_s_rtt_avg'\n",
    "X_train_rtt = train_df_rtt.drop(columns=[rtt_target_column] + exclude_rtt_columns + drop_rtt_columns, errors='ignore')\n",
    "y_train_rtt = train_df_rtt[rtt_target_column]\n",
    "X_test_rtt = test_df_rtt.drop(columns=[rtt_target_column] + exclude_rtt_columns + drop_rtt_columns, errors='ignore')\n",
    "y_test_rtt = test_df_rtt[rtt_target_column]\n",
    "\n",
    "X_train_rtt = pd.get_dummies(X_train_rtt, drop_first=True)\n",
    "X_test_rtt = pd.get_dummies(X_test_rtt, drop_first=True)\n",
    "X_train_rtt, X_test_rtt = X_train_rtt.align(X_test_rtt, join='inner', axis=1)\n",
    "\n",
    "rtt_scaler = StandardScaler()\n",
    "X_train_rtt = rtt_scaler.fit_transform(X_train_rtt)\n",
    "X_test_rtt = rtt_scaler.transform(X_test_rtt)\n",
    "\n",
    "rtt_pca = PCA(n_components=15, svd_solver='randomized')\n",
    "X_train_rtt = rtt_pca.fit_transform(X_train_rtt)\n",
    "X_test_rtt = rtt_pca.transform(X_test_rtt)\n",
    "\n",
    "# Sample dataset for faster tuning\n",
    "X_train_rtt_sample = X_train_rtt[:10000]\n",
    "y_train_rtt_sample = y_train_rtt[:10000]\n",
    "\n",
    "for model_name, param_grid in rtt_param_grids.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    model = {\n",
    "        \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(random_state=42)\n",
    "    }.get(model_name)\n",
    "\n",
    "    # Perform random search\n",
    "    random_search_rtt = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # Number of parameter combinations to try\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search_rtt.fit(X_train_rtt_sample, y_train_rtt_sample)\n",
    "\n",
    "    # Evaluate the best model\n",
    "    best_model_rtt = random_search_rtt.best_estimator_\n",
    "    y_train_pred_rtt = best_model_rtt.predict(X_train_rtt)\n",
    "    y_test_pred_rtt = best_model_rtt.predict(X_test_rtt)\n",
    "\n",
    "    train_mae_rtt = mean_absolute_error(y_train_rtt, y_train_pred_rtt)\n",
    "    test_mae_rtt = mean_absolute_error(y_test_rtt, y_test_pred_rtt)\n",
    "    train_mse_rtt = mean_squared_error(y_train_rtt, y_train_pred_rtt)\n",
    "    test_mse_rtt = mean_squared_error(y_test_rtt, y_test_pred_rtt)\n",
    "    train_r2_rtt = r2_score(y_train_rtt, y_train_pred_rtt)\n",
    "    test_r2_rtt = r2_score(y_test_rtt, y_test_pred_rtt)\n",
    "\n",
    "    rtt_tuned_results.append({\n",
    "        \"Target\": rtt_target_column,\n",
    "        \"Model\": model_name,\n",
    "        \"Best Params\": random_search_rtt.best_params_,\n",
    "        \"Train MAE\": train_mae_rtt,\n",
    "        \"Test MAE\": test_mae_rtt,\n",
    "        \"Train MSE\": train_mse_rtt,\n",
    "        \"Test MSE\": test_mse_rtt,\n",
    "        \"Train R²\": train_r2_rtt,\n",
    "        \"Test R²\": test_r2_rtt\n",
    "    })\n",
    "\n",
    "# Output tuned results\n",
    "rtt_tuned_results_df = pd.DataFrame(rtt_tuned_results)\n",
    "print(\"\\n--- Tuned Model Results for _s_rtt_avg ---\")\n",
    "print(rtt_tuned_results_df)\n"
   ],
   "id": "c161945375505b83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Random Forest...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Tuning Gradient Boosting...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "--- Tuned Model Results for _s_rtt_avg ---\n",
      "       Target              Model  \\\n",
      "0  _s_rtt_avg      Random Forest   \n",
      "1  _s_rtt_avg  Gradient Boosting   \n",
      "\n",
      "                                         Best Params   Train MAE     Test MAE  \\\n",
      "0  {'n_estimators': 200, 'min_samples_split': 2, ...  760.937978  1014.797668   \n",
      "1  {'n_estimators': 50, 'max_depth': 7, 'learning...  816.165264  1175.760867   \n",
      "\n",
      "      Train MSE      Test MSE  Train R²   Test R²  \n",
      "0  5.789805e+06  8.927203e+06  0.379016  0.113676  \n",
      "1  7.157942e+06  1.228974e+07  0.232276 -0.220169  \n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
